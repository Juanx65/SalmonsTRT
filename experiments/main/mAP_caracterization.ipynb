{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "C = 3 # number of channels of the input image\n",
    "H = 640 # heigh of the input image\n",
    "W = 640 # width of the input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mAP CARACTERIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.55 üöÄ Python-3.10.12 torch-2.3.0 CUDA:0 (Orin, 62841MiB)\n",
      "YOLOv8l-seg summary (fused): 295 layers, 45,912,659 parameters, 0 gradients, 220.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/juam/Documents/SalmonsTRT/datasets/salmons/labels/val.cache... 52 images, 34 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86/86 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/6 [00:00<?, ?it/s]/home/juam/Documents/SalmonsTRT/salmons/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/pytorch/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:07<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86        496      0.714      0.514      0.636       0.46      0.717      0.516       0.64      0.392\n",
      "Speed: 0.8ms preprocess, 66.8ms inference, 0.0ms loss, 9.2ms postprocess per image\n",
      "Results saved to \u001b[1m/home/juam/Documents/SalmonsTRT/runs/segment/val43\u001b[0m\n",
      "base model box mAP50:  0.6362255099560289\n",
      "base model box mAP50-95:  0.46046713532610867\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('../../weights/yolov8lsalmons.pt', task='segment')\n",
    "metrics = model.val(data='../../datasets/salmons/salmons.yaml', task='segment', verbose=False,conf=0.4,device='cuda')\n",
    "print('base model box mAP50: ', metrics.box.map50)\n",
    "print('base model box mAP50-95: ', metrics.box.map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRT fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('../../weights/yolov8lsalmons_fp32_bs32.engine', task='segment')\n",
    "metrics = model.val(data='../../datasets/salmons/salmons.yaml', task='segment', verbose=False,conf=0.4,device='cuda')\n",
    "print('base model box mAP50: ', metrics.box.map50)\n",
    "print('base model box mAP50-95: ', metrics.box.map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRT fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('../../weights/yolov8lsalmons_fp16_bs32.engine', task='segment')\n",
    "metrics = model.val(data='../../datasets/salmons/salmons.yaml', task='segment', verbose=False,conf=0.4,device='cuda')\n",
    "print('base model box mAP50: ', metrics.box.map50)\n",
    "print('base model box mAP50-95: ', metrics.box.map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRT int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('../../weights/yolov8lsalmons_int8_bs32.engine', task='segment')\n",
    "metrics = model.val(data='../../datasets/salmons/salmons.yaml', task='segment', verbose=False,conf=0.4,device='cuda')\n",
    "print('base model box mAP50: ', metrics.box.map50)\n",
    "print('base model box mAP50-95: ', metrics.box.map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIXEL TO PIXEL CARACTERIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare output segmentations masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = YOLO('../../weights/yolov8lsalmons.pt', task='segment')\n",
    "results_base = base_model.predict(\"../../datasets/salmons/images/val/Img2.jpeg\",show_boxes=False,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model =  YOLO('../../weights/yolov8lsalmons_fp16_bs32.engine', task='segment')\n",
    "results_trt = trt_model.predict(\"../../datasets/salmons/images/val/Img2.jpeg\", show_boxes=False,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(image1, image2):\n",
    "    # Leer im√°genes\n",
    "    img1 = cv2.imread(image1)  # Groundtruth\n",
    "    img2 = cv2.imread(image2)  # Modelo predicho\n",
    "\n",
    "    if img1.shape != img2.shape:\n",
    "        raise ValueError(\"Las im√°genes deben tener el mismo tama√±o y n√∫mero de canales\")\n",
    "\n",
    "    # Convertir im√°genes a HSV\n",
    "    hsv_img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2HSV)\n",
    "    hsv_img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Crear m√°scara de la segmentaci√≥n del groundtruth (azul)\n",
    "    lower_blue = np.array([100, 100, 50])\n",
    "    upper_blue = np.array([140, 255, 255])\n",
    "    groundtruth_mask = cv2.inRange(hsv_img1, lower_blue, upper_blue)\n",
    "\n",
    "    # Aplicar m√°scara al modelo predicho y al groundtruth\n",
    "    gt_segment = cv2.bitwise_and(img1, img1, mask=groundtruth_mask)\n",
    "    pred_segment = cv2.bitwise_and(img2, img2, mask=groundtruth_mask)\n",
    "\n",
    "    # Usar el canal azul en lugar de convertir a escala de grises\n",
    "    blue_gt = gt_segment[:, :, 0]\n",
    "    blue_pred = pred_segment[:, :, 0]\n",
    "\n",
    "    # Calcular p√≠xeles iguales dentro de la m√°scara del groundtruth\n",
    "    equal_pixels = np.sum((blue_gt == blue_pred) & (groundtruth_mask > 0))\n",
    "    total_pixels_in_mask = np.sum(groundtruth_mask > 0)\n",
    "\n",
    "    # Calcular el closeness dentro de la m√°scara\n",
    "    closeness_percentage = (equal_pixels / total_pixels_in_mask) * 100 if total_pixels_in_mask > 0 else 0\n",
    "\n",
    "    # Crear una visualizaci√≥n de diferencias\n",
    "    overlay = np.zeros_like(img1, dtype=np.uint8)\n",
    "    overlay[:, :, 2] = 255  # Canal rojo para resaltar diferencias\n",
    "\n",
    "    diferencia = cv2.absdiff(blue_gt, blue_pred)\n",
    "    _, diferencia_binaria = cv2.threshold(diferencia, 10, 255, cv2.THRESH_BINARY)\n",
    "    mask_differences = cv2.bitwise_and(overlay, overlay, mask=diferencia_binaria)\n",
    "\n",
    "    # Ajustar la mezcla para mantener el brillo original\n",
    "    img1_with_differences = cv2.addWeighted(img1, 1, mask_differences, 1, 0.0)\n",
    "\n",
    "    return img1_with_differences, closeness_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparacion_binaria, closeness = compare_images('../../outputs/segmentation/Img2_base.jpg','../../outputs/segmentation/Img2_fp32.jpg')\n",
    "cv2.imwrite('../../outputs/segmentation/Img2_fp32_compare.jpg', comparacion_binaria)\n",
    "\n",
    "# Mostrar la imagen con matplotlib\n",
    "print(\"closeness: \", closeness, '%')\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(cv2.cvtColor(comparacion_binaria, cv2.COLOR_BGR2RGB))  # Convertir de BGR a RGB para mostrar correctamente con matplotlib\n",
    "plt.axis('off')  # Ocultar ejes\n",
    "plt.title(\"Diferencias resaltadas en rojo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar objetos\n",
    "del base_model\n",
    "del trt_model\n",
    "gc.collect()\n",
    "# Limpiar cach√© de la GPU\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def combine_images_with_titles(image_paths, titles, output_path):\n",
    "    # Cargar las im√°genes\n",
    "    images = [cv2.imread(img_path) for img_path in image_paths]\n",
    "\n",
    "    # Redimensionar las im√°genes al tama√±o de la primera\n",
    "    base_height, base_width = images[0].shape[:2]\n",
    "    images = [cv2.resize(img, (base_width, base_height)) for img in images]\n",
    "\n",
    "    # Agregar t√≠tulos dentro de cada imagen\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 2\n",
    "    font_thickness = 3\n",
    "    text_color = (255, 255, 255)  # Blanco\n",
    "    text_background = (0, 0, 0)  # Negro\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        # Obtener el tama√±o del texto\n",
    "        (text_width, text_height), _ = cv2.getTextSize(titles[i], font, font_scale, font_thickness)\n",
    "        # Calcular la posici√≥n del texto (centrado en la parte superior)\n",
    "        x = (img.shape[1] - text_width) // 2\n",
    "        y = text_height + 10\n",
    "        # Dibujar un fondo negro para el texto\n",
    "        cv2.rectangle(img, (x - 5, y - text_height - 5), (x + text_width + 5, y + 5), text_background, -1)\n",
    "        # Dibujar el texto\n",
    "        cv2.putText(img, titles[i], (x, y), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "    # Crear una cuadr√≠cula de im√°genes\n",
    "    top_row = np.hstack((images[0], images[1]))\n",
    "    bottom_row = np.hstack((images[2], images[3]))\n",
    "    combined_image = np.vstack((top_row, bottom_row))\n",
    "\n",
    "    # Guardar la imagen combinada\n",
    "    cv2.imwrite(output_path, combined_image)\n",
    "    print(f\"Imagen combinada guardada en: {output_path}\")\n",
    "\n",
    "# Rutas de las im√°genes\n",
    "image_paths = [\n",
    "    \"../../outputs/segmentation/Img3_base.jpg\",\n",
    "    \"../../outputs/segmentation/Img3_fp32_compare.jpg\",\n",
    "    \"../../outputs/segmentation/Img3_fp16_compare.jpg\",\n",
    "    \"../../outputs/segmentation/Img3_int8_compare.jpg\"\n",
    "]\n",
    "\n",
    "# T√≠tulos correspondientes\n",
    "titles = [\"Modelo base\", \"TRT fp32\", \"TRT fp16\", \"TRT int8\"]\n",
    "\n",
    "# Ruta para guardar la imagen combinada\n",
    "output_path = \"../../outputs/segmentation/Img3_combined_image.jpg\"\n",
    "\n",
    "# Llamar a la funci√≥n\n",
    "combine_images_with_titles(image_paths, titles, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salmons",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
