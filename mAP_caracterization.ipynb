{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "C = 3 # number of channels of the input image\n",
    "H = 640 # heigh of the input image\n",
    "W = 640 # width of the input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mAP CARACTERIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('weights/yolov11salmons.pt', task='segment')\n",
    "metrics = model.val(data='datasets/salmons/salmons.yaml', task='segment', verbose=False,conf=0.4,device='cuda')\n",
    "print('base model box mAP50: ', metrics.box.map50)\n",
    "print('base model box mAP50-95: ', metrics.box.map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRT fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('weights/yolov11salmons_fp32_bs128_640.engine', task='segment')\n",
    "metrics = model.val(data='datasets/salmons/salmons.yaml', task='segment', verbose=False,conf=0.4,device='cuda')\n",
    "print('base model box mAP50: ', metrics.box.map50)\n",
    "print('base model box mAP50-95: ', metrics.box.map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRT fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('weights/yolov11salmons_fp16_bs128_640.engine', task='segment')\n",
    "metrics = model.val(data='datasets/salmons/salmons.yaml', task='segment', verbose=False,conf=0.4,device='cuda')\n",
    "print('base model box mAP50: ', metrics.box.map50)\n",
    "print('base model box mAP50-95: ', metrics.box.map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRT int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('weights/yolov11salmons_int8_bs128_640.engine', task='segment')\n",
    "metrics = model.val(data='datasets/salmons/salmons.yaml', task='segment', verbose=False,conf=0.4,device='cuda')\n",
    "print('base model box mAP50: ', metrics.box.map50)\n",
    "print('base model box mAP50-95: ', metrics.box.map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIXEL TO PIXEL CARACTERIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare output segmentations masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = YOLO('weights/yolov8lsalmons.pt', task='segment')\n",
    "results_base = base_model.predict(\"datasets/salmons/images/val/Img2.jpeg\",show_boxes=False,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model =  YOLO('weights/yolov8lsalmons_fp16.engine', task='segment')\n",
    "results_trt = trt_model.predict(\"datasets/salmons/images/val/Img2.jpeg\", show_boxes=False,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(image1, image2):\n",
    "    # Leer imágenes\n",
    "    img1 = cv2.imread(image1)  # Groundtruth\n",
    "    img2 = cv2.imread(image2)  # Modelo predicho\n",
    "\n",
    "    if img1.shape != img2.shape:\n",
    "        raise ValueError(\"Las imágenes deben tener el mismo tamaño y número de canales\")\n",
    "\n",
    "    # Convertir imágenes a HSV\n",
    "    hsv_img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2HSV)\n",
    "    hsv_img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Crear máscara de la segmentación del groundtruth (azul)\n",
    "    lower_blue = np.array([100, 100, 50])\n",
    "    upper_blue = np.array([140, 255, 255])\n",
    "    groundtruth_mask = cv2.inRange(hsv_img1, lower_blue, upper_blue)\n",
    "\n",
    "    # Aplicar máscara al modelo predicho y al groundtruth\n",
    "    gt_segment = cv2.bitwise_and(img1, img1, mask=groundtruth_mask)\n",
    "    pred_segment = cv2.bitwise_and(img2, img2, mask=groundtruth_mask)\n",
    "\n",
    "    # Usar el canal azul en lugar de convertir a escala de grises\n",
    "    blue_gt = gt_segment[:, :, 0]\n",
    "    blue_pred = pred_segment[:, :, 0]\n",
    "\n",
    "    # Calcular píxeles iguales dentro de la máscara del groundtruth\n",
    "    equal_pixels = np.sum((blue_gt == blue_pred) & (groundtruth_mask > 0))\n",
    "    total_pixels_in_mask = np.sum(groundtruth_mask > 0)\n",
    "\n",
    "    # Calcular el closeness dentro de la máscara\n",
    "    closeness_percentage = (equal_pixels / total_pixels_in_mask) * 100 if total_pixels_in_mask > 0 else 0\n",
    "\n",
    "    # Crear una visualización de diferencias\n",
    "    overlay = np.zeros_like(img1, dtype=np.uint8)\n",
    "    overlay[:, :, 2] = 255  # Canal rojo para resaltar diferencias\n",
    "\n",
    "    diferencia = cv2.absdiff(blue_gt, blue_pred)\n",
    "    _, diferencia_binaria = cv2.threshold(diferencia, 10, 255, cv2.THRESH_BINARY)\n",
    "    mask_differences = cv2.bitwise_and(overlay, overlay, mask=diferencia_binaria)\n",
    "\n",
    "    # Ajustar la mezcla para mantener el brillo original\n",
    "    img1_with_differences = cv2.addWeighted(img1, 1, mask_differences, 1, 0.0)\n",
    "\n",
    "    return img1_with_differences, closeness_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparacion_binaria, closeness = compare_images('outputs/segmentation/Img2_base.jpg','outputs/segmentation/Img2_fp32.jpg')\n",
    "cv2.imwrite('outputs/segmentation/Img2_fp32_compare.jpg', comparacion_binaria)\n",
    "\n",
    "# Mostrar la imagen con matplotlib\n",
    "print(\"closeness: \", closeness, '%')\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(cv2.cvtColor(comparacion_binaria, cv2.COLOR_BGR2RGB))  # Convertir de BGR a RGB para mostrar correctamente con matplotlib\n",
    "plt.axis('off')  # Ocultar ejes\n",
    "plt.title(\"Diferencias resaltadas en rojo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar objetos\n",
    "del base_model\n",
    "del trt_model\n",
    "gc.collect()\n",
    "# Limpiar caché de la GPU\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def combine_images_with_titles(image_paths, titles, output_path):\n",
    "    # Cargar las imágenes\n",
    "    images = [cv2.imread(img_path) for img_path in image_paths]\n",
    "\n",
    "    # Redimensionar las imágenes al tamaño de la primera\n",
    "    base_height, base_width = images[0].shape[:2]\n",
    "    images = [cv2.resize(img, (base_width, base_height)) for img in images]\n",
    "\n",
    "    # Agregar títulos dentro de cada imagen\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 2\n",
    "    font_thickness = 3\n",
    "    text_color = (255, 255, 255)  # Blanco\n",
    "    text_background = (0, 0, 0)  # Negro\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        # Obtener el tamaño del texto\n",
    "        (text_width, text_height), _ = cv2.getTextSize(titles[i], font, font_scale, font_thickness)\n",
    "        # Calcular la posición del texto (centrado en la parte superior)\n",
    "        x = (img.shape[1] - text_width) // 2\n",
    "        y = text_height + 10\n",
    "        # Dibujar un fondo negro para el texto\n",
    "        cv2.rectangle(img, (x - 5, y - text_height - 5), (x + text_width + 5, y + 5), text_background, -1)\n",
    "        # Dibujar el texto\n",
    "        cv2.putText(img, titles[i], (x, y), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "    # Crear una cuadrícula de imágenes\n",
    "    top_row = np.hstack((images[0], images[1]))\n",
    "    bottom_row = np.hstack((images[2], images[3]))\n",
    "    combined_image = np.vstack((top_row, bottom_row))\n",
    "\n",
    "    # Guardar la imagen combinada\n",
    "    cv2.imwrite(output_path, combined_image)\n",
    "    print(f\"Imagen combinada guardada en: {output_path}\")\n",
    "\n",
    "# Rutas de las imágenes\n",
    "image_paths = [\n",
    "    \"outputs/segmentation/Img3_base.jpg\",\n",
    "    \"outputs/segmentation/Img3_fp32_compare.jpg\",\n",
    "    \"outputs/segmentation/Img3_fp16_compare.jpg\",\n",
    "    \"outputs/segmentation/Img3_int8_compare.jpg\"\n",
    "]\n",
    "\n",
    "# Títulos correspondientes\n",
    "titles = [\"Modelo base\", \"TRT fp32\", \"TRT fp16\", \"TRT int8\"]\n",
    "\n",
    "# Ruta para guardar la imagen combinada\n",
    "output_path = \"outputs/segmentation/Img3_combined_image.jpg\"\n",
    "\n",
    "# Llamar a la función\n",
    "combine_images_with_titles(image_paths, titles, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
